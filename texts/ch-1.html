<text id="ch-1" >
	<subtitle> A brief history of graphic design </subtitle>
	During its very short official history, the field of graphic design has been extensively documented as it has evolved. Although graphic design-like activities, according to some <a id="ref-1" href="1"></a>, date back to the first cave painting, the term was only coined in the beginning of the 20th century. It first appeared in print in the 1922 essay “New Kind of Printing Calls for New Design”  by William Addison Dwiggins, an American book designer. <a id="ref-2" href="2"></a> In graphic design, “the essence is to give order to information, form to ideas, expression and feeling to artefacts that document human experience.” <a id="ref-3" href="3"></a> 
		<br><table></table>Traditional forms of graphic design relied strongly on graphic printmaking techniques as a medium, from which the initial name derived as well. Typography, way-finding systems, symbols and printed matter such as books and posters were the most typical works of a designer up to the late 20th century. Classic instances of traditional 20th century graphic design are the signage in the London Underground by Beck and Johnston <a id="ref-4" href="4"></a> <a id="ref-5" href="5"></a> and Die Neue Typographie of Jan Tschichold <a id="ref-6" href="6"></a>. 
		
		<subtitle>A new discipline</subtitle>
		Towards the mid-80s, the computer became an affordable item and thus more accessible. <a id="ref-7" href="7"></a> The graphic interface visually made the use of computers more easily accessible for the purposes of creating documents. This type of interface has been an important feature of many personal computers ever since, and is called WYSIWYG, a really long acronym for What You See Is What You Get. WYSIWYG is in fact a polished interface between a non-programmer and the computer, and gets its name from the result of the finished product: if you would print or display a page from a WYSIWYG editor, it would display just the same. 
		<br><table></table>Before the appearance of WYSIWYG, text on screen appeared in the standard system typeface—usually a monospaced font that would be made to display code—without much styling, making it hard to see the final visual outcome. If one was laying out a text, control codes had to be applied to make text bold, italic or underlined, to change the typeface or the size. <a id="ref-8" href="8"></a> That made the process of setting text digitally a painful hassle. On top of that each application had its own special way of text formatting, which made switching from one word processor to another something to be avoided. WYSIWYG made operating a computer much more enjoyable. Computers like the Apple Macintosh system even had a screen resolution that was designed to be compatible with the Apple printers of those days,—dot-matrix printers—, making the printing process faster. The image on the screen would be exactly the same size as the printed version, but with a higher resolution. <a id="ref-9" href="9"></a> The improvement of the graphic user interface of computers resulted in effortless navigation for people without too much technical knowledge—which had a significant effect on many creative practices, including graphic design. <a id="ref-9" href="9"></a> 
		<br><table></table>Many disciplines in the West were affected as a result of this innovation. The change of tools made for a change of trade. Graphic design programs often came installed on the computer, but sometimes had to be purchased separately. Programs like Netscape Composer, Quark Express and later Adobe Illustrator, inDesign and Photoshop slowly became the norm in the field. <a id="ref-10" href="10"></a>  Whereas graphic design had before relied on printmaking techniques, craftsmanship and skills, it could now be done by anybody who owned a personal desktop computer. This seemed to put traditionally educated, professional graphic designers in an uncomfortable position. In 2007, Art Chantry, a designer that worked most in the 1980s, explained the new situation: 

		<quote>“It’s never been easier to be a graphic designer: all you have to do is buy the software. But at the same time, it’s never been tougher because now everybody is a graphic designer and, for people like me it’s difficult to get work processed. It’s very frustrating. I think computers are both a good and a bad thing. Ultimately computers are going to be where the next source of amateurism and ideas come from. It is the only venue left for DIY at this point. Anybody can access a computer and build their own websites and put their shit in there and it’s all Do It Yourself. It’s too expensive to get things printed other ways now—even Kinko’s [a printer, ed.] is too fucking expensive—it’s cheaper to do it on a computer. That’s where the communication source is and that’s where the next big underground culture is going to emerge, there’s just no other option. But at the same time, it has caused enormous problems for people like me.” <a id="ref-11" href="11"></a> </quote>

		<br>Chantry argued that the arrival of the computers caused a number of complications for the professional designer. First of all, with these new tools, everybody thought they could be a designer, which caused issues for people that had studied for years, but then had to compete with a bunch of uneducated—potentially colour blind—computer nerds. On top of that, all these new designers produced an overload of amateurish design that polluted digital media and platforms. Finally, physical printing gradually became more and more expensive a medium compared to displaying the same information on-screen. Of course, for anyone who had had a career creating tangible, printed posters, this development was quite disastrous. Both Chantry’s favourite tools and media had become obsolete. The position that he took is comprehensively justifiable, but nobody seemed to care. 
		<br><table></table>On the opposite side of the spectrum of possible reactions to the development of digital media and tools, some have a nearly utopian, ultra-positive view on the role of the graphic designer in the digital era. 
		<br><table></table>Jeff Gomez, for instance, predicts  a new future in his book Print is dead <a id="ref-12" href="12"></a>. Gomez explains that apart from the people that go to the cinema for movies, hunt for CDs or cassettes in record stores to listen to music, or buy physical books, there is a phenomenon called <i>Generation Download</i>. Generation Download has no need to rummage the pages of a book for content, because they can do this through software and websites on their computer. With their headphones always plugged into at least one of their ears, their hand glued to their phones and always within the radius of WiFi reception, there is no need to leave the house to <i>experience</i>; they are in their own digital world. Gomez seems to ignore that this occurrence is not omnipresent in that generation, or that people just prefer books for the qualities they have over digital media. He says this change is the first truly transformational thing to happen in the world of words since the printing press. Therefore he incites authors, designers, distributors and readers to not only acknowledge these changes in perception, but to focus on digital creation primarily.
		<br><table></table>A more neutral proponent of new media and tools is Alessandro Ludovico. He published a book and website named Post Digital Print, in which he tries to answer the question of how digital and analogue will coexist <a id="ref-13" href="13"></a>. He shows the example of the existence of television next to the computer: its popularity may have decreased compared to the 70s, but many still have one in there homes. The same goes for radios, newspapers, printed magazines, and text messages—none of these media have ever really disappeared. They certainly evolved, were adapted to new circumstances, surrendered some of their popularity, but they have not disappeared. Ludovico feels that the death of print is an almost ideological question that surpasses the real value of publishing. To him “One of the strongest values of publishing is to develop content that can serve a community and preserve its values, intellectual elaborations and history”. Instead of opposing analogue and digital media, we should rather explore how the two can enhance each other. The difference of the media revolution is that digital media do not <i>compete</i> with other media—they swallow them and remediate them in its own form. Software has come to contain everything, images, sound, radio, movies ...
		Designers could learn from Ludovico’s perspective on media that the goal is not one medium <i>winning</i>, but rather media finding a way of functioning next to each other and strengthening each other. In the end, graphic design essentially “... gives order to information”,<a id="ref-14" href="14"></a>  no matter what medium is involved. Its significance lies in translating raw content—whether a written manuscript or a database—into an understandable form.
		<br><table></table>Lev Manovich, a media author from Moscow, has a similar opinion on the liquidity of content. In his book <i>Software Takes Command</i>, he argues: 
		<quote>'There is no such thing as <i>digital media.</i> There is only software—as applied to media (or <i>content</i>). Or, to put this differently: for users who only interact with media content through application software, the 'properties' of digital media are defined by the particular software as opposed to solely being contained in the actual content (i.e., inside digital files). <a id="ref-15" href="15"></a></quote>
		<br><table></table>Manovich here states that software is the medium that is used to give form to digital content, not the content itself. The content adapts to the digital software. This idea is similar to the phrase of academic and media theorist Marshall McLuhan <i>The medium is the message</i>.<a id="ref-16" href="16"></a> The medium essentially embeds itself in the significance of the content by influencing how this content is perceived. This impact of media differs per instance and can be actively used by designers to play with the scale and form of human association and interaction.
		<br><table></table>As a result of this shifting role of the medium, hybrid forms of media treatment appeared. This also had an impact on the role and attitude of the designer. 

		<subtitle>The Unicorn myth</subtitle>
		Now that everyone is a designer, somehow all kinds of disciplines are added to an interdisciplinary melting pot. Nowadays, real designers are Authors that do Research. They are Socially Responsible, they see their Process as design, and they think designing Rules is design--and Systems too. Designers are the Saviours Of the Modern World. Designers are Programmers. Traditional boundaries are breaking while new ones are building up.
		<br><table></table>What caused this discipline transformation? Designers seem to be in an identity crisis and to try and find new multidisciplinary niches to distinguish themselves from the vast amount of colleagues. Everybody seems to want the Rainbow Unicorn. You know: a designer that can design, program, communicate and bake red velvet cupcakes simultaneously. Rainbow unicorns are people that can craft hand-bound books with customary hand-marbled paper and handmade letterset bookmarks (hand is the essential word here), all the while developing a program that can scrape large amounts of data about kittens off the web. 
		<br><table></table>Around 2011, American designer / developer Braden Kowitz wrote an article why designers should not be developers—paradoxically, he is both himself—and baptised the persona unicorn.<a id="ref-17" href="17"></a>  This person is called a unicorn for two reasons: first of all, for its magical superpowers, secondly because it is a mythical creature of which the story has been widely spread, but which nobody has ever seen. 
		<br><table></table>“Some people can play the piano and the banjo, but when they play them both at once it sounds really crappy”, says designer Roger Belveal. <a id="ref-18" href="18"></a> The argument goes that one is better off being great at one thing,—a specialist—than mediocre at many—a Jack of all trades. Proponents state that professionalism equals ability to stay focused on one thing. For a successful outcome, people with different perspectives are needed, they can work together and stay focused on their greatest strengths. 
		Taken for granted in this concept of collaboration between designer and programmer is the productive performance of the process, the readiness of the company to pay two wages for one job and the notion of the two not getting to the point of smashing their laptops to pieces on each other’s heads. In order to be able to communicate, it is useful to have a common ground of communication. <i>A unified process often is more structured, less inconsistent and finished faster.</i>, states American designer David Cole. <a id="ref-19" href="19"></a>
		<br><table></table>However, design already is not a single skill. There are so many techniques, programs in and so many approaches to the field that it is hard to describe what design actually is to an outsider. Why is programming thus rejected in such a way? The combination of multiple skills can intensify both. A great example of combination of skills is DrawBot, <a id="ref-20" href="20"></a> a tool for visuals based on the programming language Python, created by Just van Rossum, Erik van Blokland and Frederik Berlaen. As a professor at the Royal Academy of The Hague, Van Rossum teaches Python to graphic design and Type&Media students using DrawBot as interface. <a id="ref-21" href="21"></a> Since the classes have typography as subject, student projects often result in dynamic typefaces or letterforms that could never be made with typographic craftsmanship only. 

		<subtitle>Cognition	</subtitle>
		Learning is not a scale activity. By writing JavaScript, one does not suddenly forget how to kern type. The statement that learning multiple tasks results in a mediocre performance of all, disregards this as well as the fact that lifelong learning is not a disposition that everybody enjoys. 
		<br><table></table>Humans begin learning at birth and continue to do so throughout life, but how much is learned and the value of the acquired knowledge varies per individual. <a id="ref-22" href="22"></a> However, from the age of 25, human ability to learn declines with about 1% per year. <a id="ref-23" href="23"></a> Whereas programming has more and more frequently become a part of the primary school educational curriculum, this was rarely the case before 2012. <a id="ref-24" href="24"></a> The generation of the late 80s and 90s is in an interesting position; they have not been taught programming as a basis of their education, yet got in contact with the discipline in their teenage years or twenties. Current design students often have learned design practice before programming and have to bridge a gap between a generation that mainly worked without the use of programming and a generation where programming is a part of their primary knowledge.
		<br><table></table>Andragogy is a theory about lifelong learning, coined by German educator Alexander Kapp in 1833. It derives from the Greek aνδρός (man) and aγω (to lead). <a id="ref-25" href="25"></a>: In 1968, American educator Malcolm Knowles juxtaposed andragogy to <i>taught</i> learning as a term for <i>self-direction’</i> Self-directed cognition, Knowles states, can only take place as a sum of an individual’s inner desire to learn, the relation of the matter to one’s daily life and previous experiences. It is best executed in an informal environment with a certain guidance and a direct possibility for practical application, learning by doing. <a id="ref-26" href="25"></a> Due to all these conditions, it is sooner an exception than the rule that schooled adults learn a new practice. It rarely happens that extra time is actually spent with such a practical activity as learning programming, if inner motivation is missing. Yet, one does not un-learn by learning new practices. 

		<subtitle>Redefining craftsmanship</subtitle>
		Nevertheless, there are similarities between the programming and design disciplines. Both are seen by some as a craft. A craft is an occupation or trade requiring manual dexterity and skills. <a id="ref-26" href="26"></a> Originally, this would apply to manual crafts, a craftsman would originally be <i>a member of a craft guild</i>, which were groups of carpenters, printers or painters.
		<br><table></table>But with the rise of new media, <i>manual</i> lost its value. The term craftsmanship was appropriated by The Software Craftsmanship Manifesto,<a id="ref-27" href="27"></a> who made a metaphorical comparison between modern software development and the apprenticeship model of these guilds, where younger apprentices would learn from their masters and take over the business when the time was ripe. Creating a piece of software demands skills, structure and attention to detail just as much as creating a well-designed book does. Even the design principles work similarly, although the approach might be different, interfaces need a similar treatment, where text hierarchy, balance and white space play an important role. Programming is a craft, too.
		<br><table></table>In 2001 Ben Fry and Casey Reas, two PhD students at the Massachusetts Institute of Technology,  developed Processing, <a id="ref-28" href="28"></a> an open source programming language especially for design, electronic arts and new media art, with the purpose of teaching fundamentals of programming in a visual manner. Over the years, Processing has gained vast amounts of popularity. The current possibilities are endless. One can do anything from drawing a circle to making a complex 3D model of a record with your favourite song. By bridging visual arts, design and programming, Processing is an artistic expression of digital craftsmanship.
</text>